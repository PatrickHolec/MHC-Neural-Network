{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "Project: Neural Network for MHC Peptide Prediction\n",
    "Class(s): BuildNetwork\n",
    "Function: Generates specified neural network architecture (data agnostic)\n",
    "\n",
    "Author: Patrick V. Holec\n",
    "Date Created: 2/2/2017\n",
    "Date Updated: 2/2/2017\n",
    "'''\n",
    "\n",
    "\n",
    "# standard libaries\n",
    "import gzip\n",
    "import math\n",
    "import os.path\n",
    "import time\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "# nonstandard libraries\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# library modifications\n",
    "random.seed(42)\n",
    "tf.set_random_seed(42)\n",
    "\n",
    "# main test function\n",
    "def main():\n",
    "    pass\n",
    "\n",
    "# Factory methods for creating variables and layers\n",
    "\n",
    "def weight_variable(shape):\n",
    "    \"\"\"Create a weight variable with appropriate initialization.\"\"\"\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    \"\"\"Create a bias variable with appropriate initialization.\"\"\"\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W, stride=1, padding='SAME'):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, stride, stride, 1], padding=padding)\n",
    "\n",
    "def max_pool(x, stride=2, filter_size=2):\n",
    "    return tf.nn.max_pool(x, ksize=[1, filter_size, filter_size, 1],\n",
    "                        strides=[1, stride, stride, 1], padding='SAME')\n",
    "\n",
    "def cross_entropy(y, y_real):\n",
    "    return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y, y_real))\n",
    "\n",
    "def l2_loss(y,y_real):\n",
    "    return \n",
    "    #return tf.nn.l2_loss(y - y_real)\n",
    "    \n",
    "def build_two_fc_layers(x_inp,Ws,bs):\n",
    "    h_fc1 = tf.nn.relu(tf.matmul(x_inp, Ws[0]) + bs[0])\n",
    "    return tf.matmul(h_fc1, Ws[1]) + bs[1]\n",
    "\n",
    "'''\n",
    "BuildNetwork: Main neural network architecture generator\n",
    "'''\n",
    "\n",
    "class BuildNetwork:\n",
    "    def __init__(self,label=None,silent=False):\n",
    "        print 'Initializing neural network data acquisition...'\n",
    "        \n",
    "        # check for label\n",
    "        if not label:\n",
    "            print 'No data label defined, unable to initialize any architecture.'\n",
    "            return None\n",
    "        \n",
    "        # load data parameters\n",
    "        data = LoadedData(label)\n",
    "        self.__dict__.update(data.params)\n",
    "        self.all_data,self.all_labels = data.data_array,data.label_array\n",
    "        \n",
    "        # set some basic hyperparameters\n",
    "        self.test_fraction = 0.2\n",
    "        self.batch_size = 10\n",
    "        self.x_dim,self.y_dim = self.aa_count,self.length\n",
    "        \n",
    "        # update on model parameters\n",
    "        if not silent:\n",
    "            print '*** System Parameters'\n",
    "            print '  - Sequence length:',self.length\n",
    "            print '  - AA count:',self.aa_count\n",
    "        \n",
    "        print 'Finished acquisition!'\n",
    "    \n",
    "    def data_format(self,silent=False,**kwargs):\n",
    "        print 'Starting data formatting...'\n",
    "        \n",
    "        # randomize data order\n",
    "        order,limit = range(0,len(self.all_data)),int((1-self.test_fraction)*len(self.all_data))\n",
    "        np.array(random.shuffle(order))\n",
    "        \n",
    "        # normalize label energy\n",
    "        self.all_labels = np.reshape(np.array([(self.all_labels - min(self.all_labels))/\n",
    "                                               (max(self.all_labels)-min(self.all_labels))]),\n",
    "                                     (len(self.all_labels),1,1))\n",
    "        \n",
    "        # split data into training and testing\n",
    "        self.train_data = self.all_data[np.array(order[:limit]),:,:]\n",
    "        self.test_data = self.all_data[np.array(order[limit:]),:,:]\n",
    "        self.train_labels = self.all_labels[np.array(order[:limit]),:,:]\n",
    "        self.test_labels = self.all_labels[np.array(order[limit:]),:,:]\n",
    "        \n",
    "        self.train_data = np.reshape(self.train_data,(self.train_data.shape[0],self.x_dim*self.y_dim))\n",
    "        self.test_data = np.reshape(self.test_data,(self.test_data.shape[0],self.x_dim*self.y_dim))\n",
    "        self.train_labels = np.reshape(self.train_labels,(self.train_labels.shape[0],1))\n",
    "        self.test_labels = np.reshape(self.test_labels,(self.test_labels.shape[0],1))\n",
    "        \n",
    "        print 'Finished formatting!'\n",
    "\n",
    "\n",
    "    ''' \n",
    "    Provides output class variables:\n",
    "    self.train_x,self.train_y\n",
    "    '''\n",
    "    \n",
    "    def CNN_initialization(self,layers=2,learning_rate=0.01):\n",
    "        print 'Constructing CNN graph...'\n",
    "        \n",
    "        # model hyperparameters\n",
    "        conv1_filter_size = (self.aa_count,1)\n",
    "        conv1_depth = self.aa_count # a filter for each position!\n",
    "        conv2_filter_size = (3,3)\n",
    "        conv2_depth = 5\n",
    "        conv_stride = 1\n",
    "        fc_num_hidden = self.aa_count*self.length\n",
    "        \n",
    "        # create model input structure\n",
    "        self.train_x = tf.placeholder(tf.float32, shape=(None, self.x_dim*self.y_dim)) # vector input\n",
    "        self.train_y = tf.placeholder(tf.float32, shape=(None, 1)) # one output (ddG)\n",
    "        x_image = tf.reshape(self.train_x, [-1, self.x_dim, self.y_dim, 1])\n",
    "        \n",
    "        # weight initialization\n",
    "        self.W1 = weight_variable([conv1_filter_size[0],conv1_filter_size[1],1,conv1_depth])\n",
    "        self.W2 = weight_variable([conv2_filter_size[0],conv2_filter_size[1],conv1_depth,conv2_depth])\n",
    "        \n",
    "\n",
    "        \n",
    "        # convolutional layer 1\n",
    "        conv1 = conv2d(x_image, self.W1, stride=conv_stride,padding='VALID') # keeps dimensions at [x_dim,y_dim]\n",
    "        #h_pool1 = max_pool(conv1, stride=2, filter_size=2) # keeps dimensions at [x_dim,y_dim]\n",
    "        \n",
    "        # convolutional layer 2\n",
    "        conv2 = conv2d(conv1, self.W2, stride=conv_stride,padding='SAME')\n",
    "        #h_pool2 = max_pool(conv2, stride=2, filter_size=2)\n",
    "        \n",
    "        # size of feature maps\n",
    "        conv2_feat_map_x = int(conv2.get_shape()[2])   # Define the x-size of the conv2 feature map\n",
    "        conv2_feat_map_y = int(conv2.get_shape()[1])   # Define the y-size of the conv2 feature map\n",
    "\n",
    "        # weights/biases for fully connected layer 1\n",
    "        self.W_fc1 = weight_variable([conv2_feat_map_x * conv2_feat_map_y * conv2_depth, fc_num_hidden])\n",
    "        self.b_fc1 = bias_variable([fc_num_hidden])\n",
    "        \n",
    "        h_pool2_flat = tf.reshape(conv2, [-1, conv2_feat_map_x * conv2_feat_map_y * conv2_depth])\n",
    "        \n",
    "        # weights/biases for fully connected layer 2\n",
    "        self.W_fc2 = weight_variable([fc_num_hidden, 1])\n",
    "        self.b_fc2 = bias_variable([1])\n",
    "\n",
    "        self.y_conv = build_two_fc_layers(h_pool2_flat, [self.W_fc1, self.W_fc2], [self.b_fc1, self.b_fc2])\n",
    "        \n",
    "        # model access variables\n",
    "        #self.loss = cross_entropy(self.y_conv, self.train_y)\n",
    "        self.loss = l2_loss(self.y_conv,self.train_y)\n",
    "        self.train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(self.loss)\n",
    "        \n",
    "        print 'Finished construction!'\n",
    "        \n",
    "    \n",
    "    # requires self.train_data,self.train_labels\n",
    "    #          self.train_step,self.loss\n",
    "    def train(self):\n",
    "        # start timer\n",
    "        start = time.time()\n",
    "        \n",
    "        # training hyperparameters\n",
    "        num_epochs = 500\n",
    "        batch_size = 50\n",
    "\n",
    "        # training via iterative epochs\n",
    "        batches_per_epoch = int(len(self.train_data)/batch_size)\n",
    "        num_steps = int(num_epochs * batches_per_epoch)\n",
    "        \n",
    "        print 'Batchs per epoch - {} / Number of steps - {}'.format(batches_per_epoch,num_steps)\n",
    "        \n",
    "        sess = tf.Session()\n",
    "        init = tf.global_variables_initializer()\n",
    "        sess.run(init)\n",
    "        print 'Initializing variables...'\n",
    "\n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "        for step in xrange(num_steps):\n",
    "            offset = (step * batch_size) % (self.train_data.shape[0] - batch_size)\n",
    "\n",
    "            batch_x = self.train_data[offset:(offset + batch_size), :]\n",
    "            batch_y = self.train_labels[offset:(offset + batch_size)]\n",
    "\n",
    "            feed_dict = {self.train_x: batch_x, self.train_y: batch_y}\n",
    "            \n",
    "            _, batch_loss = sess.run([self.train_step, self.loss],feed_dict=feed_dict)\n",
    "            \n",
    "            epoch_loss += batch_loss\n",
    "                        \n",
    "            if (step % batches_per_epoch == 0):\n",
    "                epoch_loss /= batches_per_epoch\n",
    "                print 'Avg batch loss at step %d: %f' % (step, epoch_loss)\n",
    "                epoch_loss = 0\n",
    "                # randomize input data\n",
    "                together = np.concatenate((self.train_data,self.train_labels),axis=1)\n",
    "                np.random.shuffle(together)\n",
    "                self.train_data = together[:,:-1]\n",
    "                self.train_labels = np.reshape(together[:,-1],(self.train_labels.shape[0],1)) # need to add dimension to data\n",
    "                \n",
    "        print \"Training time: \", time.time() - start\n",
    "        \n",
    "        sess.close()\n",
    "        \n",
    "    def visualization(self,picks=['test_accuracy','filters']):\n",
    "        # start engine\n",
    "        sess = tf.Session()\n",
    "        init = tf.global_variables_initializer()\n",
    "        sess.run(init)\n",
    "        \n",
    "        if 'test_accuracy' in picks:\n",
    "            # visualize\n",
    "            predicted_labels = sess.run(self.y_conv, feed_dict={self.train_x: self.test_data})\n",
    "            plt.scatter(predicted_labels,self.test_labels)\n",
    "            plt.show()\n",
    "            \n",
    "        if 'filters' in picks:\n",
    "            # layer 1 weights\n",
    "            A = sess.run(self.W1)\n",
    "            print 'A:',A\n",
    "            sh = A.shape\n",
    "            A = np.reshape(A,(sh[0],sh[3]))\n",
    "            if True:\n",
    "                plt.imshow(A, cmap='jet', interpolation='nearest')\n",
    "                plt.title('Filter (Layer 1)')\n",
    "                plt.xlabel('AA Index')\n",
    "                plt.ylabel('Filter #')\n",
    "                plt.show()   \n",
    "                \n",
    "        sess.close()\n",
    "        \n",
    "        \n",
    "class LoadedData:\n",
    "    def __init__(self,label='test'):\n",
    "        \n",
    "        # open file and store lines\n",
    "        with open('{}_seqs.txt'.format(label),'r') as f:\n",
    "            content = f.readlines()\n",
    "        \n",
    "        # pull out parameters from the pickeled params file\n",
    "        self.params = pickle.load(open('{}_params.p'.format(label),'rb'))\n",
    "        \n",
    "        # split up lines it data and labels\n",
    "        self.raw_data,self.raw_labels = [],[]\n",
    "        for data in [c.strip('\\n').split(',') for c in content]:\n",
    "            self.raw_data.append(data[0])\n",
    "            self.raw_labels.append(float(data[1]))\n",
    "            \n",
    "        # one-hot encoding\n",
    "        self.data_array = np.zeros((len(self.raw_data),self.params['aa_count'],self.params['length']),np.int)\n",
    "        self.label_array = np.reshape(np.array(self.raw_labels),(len(self.raw_labels),1,1))\n",
    "        # np.zeros((len(self.raw_labels),1,1),np.float32)\n",
    "        for i,sample in enumerate(self.raw_data):\n",
    "            for j,char in enumerate(sample):\n",
    "                self.data_array[i,self.params['characters'].index(char),j] = 1\n",
    "            \n",
    "# namespace activation\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing neural network data acquisition...\n",
      "*** System Parameters\n",
      "  - Sequence length: 5\n",
      "  - AA count: 6\n",
      "Finished acquisition!\n",
      "Starting data formatting...\n",
      "Finished formatting!\n",
      "Constructing CNN graph...\n",
      "W1: (6, 1, 1, 6)\n",
      "X_image: (?, 6, 5, 1)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'dtype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-f3ce9415d615>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnetwork\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBuildNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCNN_initialization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.00001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisualization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-7ff7dcfc2c88>\u001b[0m in \u001b[0;36mCNN_initialization\u001b[0;34m(self, layers, learning_rate)\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;31m#self.loss = cross_entropy(self.y_conv, self.train_y)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ml2_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_conv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientDescentOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0;34m'Finished construction!'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.pyc\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(self, loss, global_step, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, name, grad_loss)\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[0maggregation_method\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation_method\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0mcolocate_gradients_with_ops\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolocate_gradients_with_ops\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m         grad_loss=grad_loss)\n\u001b[0m\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[0mvars_with_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrads_and_vars\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mg\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.pyc\u001b[0m in \u001b[0;36mcompute_gradients\u001b[0;34m(self, loss, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, grad_loss)\u001b[0m\n\u001b[1;32m    328\u001b[0m                        \u001b[0;34m\"Optimizer.GATE_OP, Optimizer.GATE_GRAPH.  Not %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m                        gate_gradients)\n\u001b[0;32m--> 330\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_assert_valid_dtypes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    331\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mgrad_loss\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_assert_valid_dtypes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgrad_loss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.pyc\u001b[0m in \u001b[0;36m_assert_valid_dtypes\u001b[0;34m(self, tensors)\u001b[0m\n\u001b[1;32m    468\u001b[0m     \u001b[0mvalid_dtypes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_valid_dtypes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 470\u001b[0;31m       \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    471\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalid_dtypes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m         raise ValueError(\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'dtype'"
     ]
    }
   ],
   "source": [
    "network = BuildNetwork('test')\n",
    "network.data_format()\n",
    "network.CNN_initialization(learning_rate=0.00001)\n",
    "network.train()\n",
    "network.visualization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
