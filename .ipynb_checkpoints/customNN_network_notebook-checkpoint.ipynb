{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Project: Neural Network for MHC Peptide Prediction\n",
    "Class(s): BuildNetwork\n",
    "Function: Gssssnerates specified neural network architecture (data agnostic)\n",
    "\n",
    "Author: Patrick V. Holec\n",
    "Date Created: 2/2/2017\n",
    "Date Updated: 2/2/2017\n",
    "'''\n",
    "\n",
    "\n",
    "# standard libaries\n",
    "import gzip\n",
    "import math\n",
    "import os.path\n",
    "import time\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "# nonstandard libraries\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# library modifications\n",
    "random.seed(42)\n",
    "tf.set_random_seed(42)\n",
    "\n",
    "# main test function\n",
    "def main():\n",
    "    pass\n",
    "\n",
    "# Factory methods for creating variables and layers\n",
    "\n",
    "def weight_variable(shape):\n",
    "    \"\"\"Create a weight variable with appropriate initialization.\"\"\"\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    \"\"\"Create a bias variable with appropriate initialization.\"\"\"\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W, stride=1, padding='SAME'):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, stride, stride, 1], padding=padding)\n",
    "\n",
    "def max_pool(x, stride=2, filter_size=2):\n",
    "    return tf.nn.max_pool(x, ksize=[1, filter_size, filter_size, 1],\n",
    "                        strides=[1, stride, stride, 1], padding='SAME')\n",
    "\n",
    "def cross_entropy(y, y_real):\n",
    "    return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y, y_real))\n",
    "\n",
    "def l2_loss(y,y_real):\n",
    "    return \n",
    "    #return tf.nn.l2_loss(y - y_real)\n",
    "    \n",
    "def build_two_fc_layers(x_inp,Ws,bs):\n",
    "    h_fc1 = tf.nn.relu(tf.matmul(x_inp, Ws[0]) + bs[0])\n",
    "    return tf.matmul(h_fc1, Ws[1]) + bs[1]\n",
    "\n",
    "'''\n",
    "BuildNetwork: Main neural network architecture generator\n",
    "'''\n",
    "\n",
    "class BuildCustomNetwork:\n",
    "    def __init__(self,label=None,silent=False):\n",
    "        print 'Initializing neural network data acquisition...'\n",
    "        \n",
    "        # check for label\n",
    "        if not label:\n",
    "            print 'No data label defined, unable to initialize any architecture.'\n",
    "            return None\n",
    "        \n",
    "        # load data parameters\n",
    "        data = LoadedData(label)\n",
    "        self.__dict__.update(data.params)\n",
    "        self.all_data_sw,self.all_data_pw = data.data_array_sw,data.data_array_pw # load all variables\n",
    "        self.all_labels = data.label_array\n",
    "        \n",
    "        # set some basic hyperparameters\n",
    "        self.test_fraction = 0.2\n",
    "        self.batch_size = 10\n",
    "        \n",
    "        #self.x_dim,self.y_dim = self.aa_count,self.length # maybe outdated\n",
    "        self.sw_dim,self.pw_dim = self.all_data_sw.shape,self.all_data_pw.shape # save dimensions of original data\n",
    "        self.full_size = self.all_data_sw.size + self.all_data_pw.size\n",
    "        \n",
    "        # verified reduction of dimension and merging\n",
    "        self.flatten_data_sw = np.reshape(self.all_data_sw,\n",
    "                                          (self.pw_dim[0],self.sw_dim[1]*self.sw_dim[2]))\n",
    "        self.flatten_data_pw = np.reshape(self.all_data_pw,\n",
    "                                          (self.pw_dim[0],self.pw_dim[1]*self.pw_dim[2]*self.pw_dim[3]))\n",
    "        self.all_data = np.concatenate((self.flatten_data_sw,self.flatten_data_pw),axis=1)\n",
    "        \n",
    "        # update on model parameters\n",
    "        if not silent:\n",
    "            print '*** System Parameters ***'\n",
    "            print '  - Sequence length:',self.length\n",
    "            print '  - AA count:',self.aa_count\n",
    "        \n",
    "        print 'Finished acquisition!'\n",
    "    \n",
    "    def data_format(self,silent=False,**kwargs):\n",
    "        print 'Starting data formatting...'\n",
    "        \n",
    "        # randomize data order\n",
    "        order,limit = range(0,self.all_data.shape[0]),int((1-self.test_fraction)*self.all_data.shape[0])\n",
    "        np.array(random.shuffle(order))\n",
    "        \n",
    "        # normalize label energy\n",
    "        self.all_labels = np.reshape(np.array([(self.all_labels - min(self.all_labels))/\n",
    "                                               (max(self.all_labels)-min(self.all_labels))]),(len(self.all_labels),1,1))\n",
    "        \n",
    "        # split data into training and testing\n",
    "        self.train_data = self.all_data[np.array(order[:limit]),:]\n",
    "        self.test_data = self.all_data[np.array(order[limit:]),:]\n",
    "        self.train_labels = self.all_labels[np.array(order[:limit]),:]\n",
    "        self.test_labels = self.all_labels[np.array(order[limit:]),:]\n",
    "        \n",
    "        print 'Train data:',self.train_data.shape\n",
    "        print 'Test data:',self.test_data.shape\n",
    "        print 'Train labels:',self.train_labels.shape\n",
    "        print 'Train labels:',self.test_labels.shape\n",
    "        \n",
    "        print np.reshape(np.transpose(self.train_data[0,:self.length*self.aa_count]),(self.,5))\n",
    "        print np.reshape(self.train_data[0,self.length*self.aa_count:],(10,5,5))\n",
    "\n",
    "        print 'Finished formatting!'\n",
    "\n",
    "\n",
    "    ''' \n",
    "    Provides output class variables:\n",
    "    self.train_x,self.train_y\n",
    "    '''\n",
    "    \n",
    "    def network_initialization(self,layers=2,learning_rate=0.01):\n",
    "        print 'Constructing CNN graph...'\n",
    "        \n",
    "        # model hyperparameters\n",
    "        conv1_filter_size = (self.aa_count,1)\n",
    "        conv1_depth = self.aa_count # a filter for each position!\n",
    "        conv2_filter_size = (3,3)\n",
    "        conv2_depth = 5\n",
    "        conv_stride = 1\n",
    "        fc_num_hidden = self.aa_count*self.length\n",
    "        \n",
    "        # create model input structure, NOTE: added _sw/_pw extension\n",
    "        self.train_x_sw = tf.placeholder(tf.float32, shape=(None, self.x_dim*self.y_dim)) # vector input\n",
    "        self.train_x_pw = tf.placeholder(tf.float32, shape=(None, self.x_dim*self.y_dim)) # vector input\n",
    "\n",
    "        self.train_y = tf.placeholder(tf.float32, shape=(None, 1)) # one output (ddG)\n",
    "        x_image_sw = tf.reshape(self.train_x, [-1, self.x_dim, self.y_dim, 1])\n",
    "        x_image_pw = tf.reshape(self.train_x, [-1, self.x_dim, self.y_dim, 1])        \n",
    "        \n",
    "        # weight initialization\n",
    "        self.W1 = weight_variable([conv1_filter_size[0],conv1_filter_size[1],1,conv1_depth])\n",
    "        self.W2 = weight_variable([conv2_filter_size[0],conv2_filter_size[1],conv1_depth,conv2_depth])\n",
    "                \n",
    "        # convolutional layer 1\n",
    "        conv1 = conv2d(x_image, self.W1, stride=conv_stride,padding='VALID') # keeps dimensions at [x_dim,y_dim]\n",
    "        #h_pool1 = max_pool(conv1, stride=2, filter_size=2) # keeps dimensions at [x_dim,y_dim]\n",
    "        \n",
    "        # convolutional layer 2\n",
    "        conv2 = conv2d(conv1, self.W2, stride=conv_stride,padding='SAME')\n",
    "        #h_pool2 = max_pool(conv2, stride=2, filter_size=2)\n",
    "        \n",
    "        # size of feature maps\n",
    "        conv2_feat_map_x = int(conv2.get_shape()[2])   # Define the x-size of the conv2 feature map\n",
    "        conv2_feat_map_y = int(conv2.get_shape()[1])   # Define the y-size of the conv2 feature map\n",
    "\n",
    "        # weights/biases for fully connected layer 1\n",
    "        self.W_fc1 = weight_variable([conv2_feat_map_x * conv2_feat_map_y * conv2_depth, fc_num_hidden])\n",
    "        self.b_fc1 = bias_variable([fc_num_hidden])\n",
    "        \n",
    "        h_pool2_flat = tf.reshape(conv2, [-1, conv2_feat_map_x * conv2_feat_map_y * conv2_depth])\n",
    "        \n",
    "        # weights/biases for fully connected layer 2\n",
    "        self.W_fc2 = weight_variable([fc_num_hidden, 1])\n",
    "        self.b_fc2 = bias_variable([1])\n",
    "\n",
    "        self.y_conv = build_two_fc_layers(h_pool2_flat, [self.W_fc1, self.W_fc2], [self.b_fc1, self.b_fc2])\n",
    "        \n",
    "        # model access variables\n",
    "        #self.loss = cross_entropy(self.y_conv, self.train_y)\n",
    "        self.loss = l2_loss(self.y_conv,self.train_y)\n",
    "        self.train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(self.loss)\n",
    "        \n",
    "        print 'Finished construction!'\n",
    "        \n",
    "    \n",
    "    # requires self.train_data,self.train_labels\n",
    "    #          self.train_step,self.loss\n",
    "    def train(self):\n",
    "        # start timer\n",
    "        start = time.time()\n",
    "        \n",
    "        # training hyperparameters\n",
    "        num_epochs = 500\n",
    "        batch_size = 50\n",
    "\n",
    "        # training via iterative epochs\n",
    "        batches_per_epoch = int(len(self.train_data)/batch_size)\n",
    "        num_steps = int(num_epochs * batches_per_epoch)\n",
    "        \n",
    "        print 'Batchs per epoch - {} / Number of steps - {}'.format(batches_per_epoch,num_steps)\n",
    "        \n",
    "        sess = tf.Session()\n",
    "        init = tf.global_variables_initializer()\n",
    "        sess.run(init)\n",
    "        print 'Initializing variables...'\n",
    "\n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "        for step in xrange(num_steps):\n",
    "            offset = (step * batch_size) % (self.train_data.shape[0] - batch_size)\n",
    "\n",
    "            batch_x = self.train_data[offset:(offset + batch_size), :]\n",
    "            batch_y = self.train_labels[offset:(offset + batch_size)]\n",
    "\n",
    "            feed_dict = {self.train_x: batch_x, self.train_y: batch_y}\n",
    "            \n",
    "            _, batch_loss = sess.run([self.train_step, self.loss],feed_dict=feed_dict)\n",
    "            \n",
    "            epoch_loss += batch_loss\n",
    "                        \n",
    "            if (step % batches_per_epoch == 0):\n",
    "                epoch_loss /= batches_per_epoch\n",
    "                print 'Avg batch loss at step %d: %f' % (step, epoch_loss)\n",
    "                epoch_loss = 0\n",
    "                # randomize input data\n",
    "                together = np.concatenate((self.train_data,self.train_labels),axis=1)\n",
    "                np.random.shuffle(together)\n",
    "                self.train_data = together[:,:-1]\n",
    "                self.train_labels = np.reshape(together[:,-1],(self.train_labels.shape[0],1)) # need to add dimension to data\n",
    "                \n",
    "        print \"Training time: \", time.time() - start\n",
    "        \n",
    "        sess.close()\n",
    "        \n",
    "    def visualization(self,picks=['test_accuracy','filters']):\n",
    "        # start engine\n",
    "        sess = tf.Session()\n",
    "        init = tf.global_variables_initializer()\n",
    "        sess.run(init)\n",
    "        \n",
    "        if 'test_accuracy' in picks:\n",
    "            # visualize\n",
    "            predicted_labels = sess.run(self.y_conv, feed_dict={self.train_x: self.test_data})\n",
    "            plt.scatter(predicted_labels,self.test_labels)\n",
    "            plt.show()\n",
    "            \n",
    "        if 'filters' in picks:\n",
    "            # layer 1 weights\n",
    "            A = sess.run(self.W1)\n",
    "            print 'A:',A\n",
    "            sh = A.shape\n",
    "            A = np.reshape(A,(sh[0],sh[3]))\n",
    "            if True:\n",
    "                plt.imshow(A, cmap='jet', interpolation='nearest')\n",
    "                plt.title('Filter (Layer 1)')\n",
    "                plt.xlabel('AA Index')\n",
    "                plt.ylabel('Filter #')\n",
    "                plt.show()   \n",
    "                \n",
    "        sess.close()\n",
    "        \n",
    "        \n",
    "class LoadedData:\n",
    "    def __init__(self,label='test',scope=['sw','pw']):\n",
    "\n",
    "        # hold variable names (in case undefined)\n",
    "        self.data_array_sw,self.data_array_pw,self.label_array = None,None,None\n",
    "        \n",
    "        # open file and store lines\n",
    "        with open('{}_seqs.txt'.format(label),'r') as f:\n",
    "            content = f.readlines()\n",
    "        \n",
    "        # pull out parameters from the pickeled params file\n",
    "        self.params = pickle.load(open('{}_params.p'.format(label),'rb'))\n",
    "        \n",
    "        # split up lines it data and labels\n",
    "        self.raw_data,self.raw_labels = [],[]\n",
    "        for data in [c.strip('\\n').split(',') for c in content]:\n",
    "            self.raw_data.append(data[0])\n",
    "            self.raw_labels.append(float(data[1]))\n",
    "        \n",
    "        # always make label array\n",
    "        self.label_array = np.reshape(np.array(self.raw_labels),(len(self.raw_labels),1,1))\n",
    "            \n",
    "        # one-hot encoding (sitewise)\n",
    "        if 'sw' in scope:\n",
    "            self.data_array_sw = np.zeros((len(self.raw_data),self.params['aa_count'],self.params['length']),np.int)\n",
    "            for i,sample in enumerate(self.raw_data):\n",
    "                for j,char in enumerate(sample):\n",
    "                    self.data_array_sw[i,self.params['characters'].index(char),j] = 1\n",
    "                \n",
    "        # one-hot encoding (pairwise)\n",
    "        if 'pw' in scope:\n",
    "            pair_count = (self.params['length']*(self.params['length']-1))/2\n",
    "            self.data_array_pw = np.zeros((len(self.raw_data),pair_count,self.params['aa_count'],self.params['aa_count']))\n",
    "            for i,sample in enumerate(self.raw_data):\n",
    "                pair_index = 0\n",
    "                for j,char1 in enumerate(sample):\n",
    "                    for k,char2 in enumerate(sample[j+1:]):\n",
    "                        self.data_array_pw[i,pair_index,self.params['characters'].index(char1),\n",
    "                                                        self.params['characters'].index(char2)] = 1\n",
    "                        pair_index += 1\n",
    "\n",
    "            \n",
    "# namespace activation\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing neural network data acquisition...\n",
      "*** System Parameters ***\n",
      "  - Sequence length: 5\n",
      "  - AA count: 5\n",
      "Finished acquisition!\n",
      "Starting data formatting...\n",
      "Train data: (688, 275)\n",
      "Test data: (173, 275)\n",
      "Train labels: (688, 1, 1)\n",
      "Train labels: (173, 1, 1)\n",
      "[[ 1.  0.  1.  0.  0.]\n",
      " [ 0.  1.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.]]\n",
      "[[[ 0.  1.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.]]\n",
      "\n",
      " [[ 1.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.]]\n",
      "\n",
      " [[ 0.  0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.]]\n",
      "\n",
      " [[ 0.  1.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.]]\n",
      "\n",
      " [[ 0.  0.  0.  0.  0.]\n",
      "  [ 1.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.]]\n",
      "\n",
      " [[ 0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.]]\n",
      "\n",
      " [[ 0.  0.  0.  0.  0.]\n",
      "  [ 0.  1.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.]]\n",
      "\n",
      " [[ 0.  0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.]]\n",
      "\n",
      " [[ 0.  1.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.]]\n",
      "\n",
      " [[ 0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.]\n",
      "  [ 0.  1.  0.  0.  0.]]]\n",
      "Finished formatting!\n"
     ]
    }
   ],
   "source": [
    "network = BuildCustomNetwork('test')\n",
    "network.data_format()\n",
    "#network.network_initialization(learning_rate=0.00001)\n",
    "#network.train()\n",
    "#network.visualization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'limit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-fda5654fdbba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mprint\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'limit' is not defined"
     ]
    }
   ],
   "source": [
    "print limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
