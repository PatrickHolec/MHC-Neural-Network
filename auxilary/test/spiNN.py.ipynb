{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# coding: utf-8\n",
    "\n",
    "# In[32]:\n",
    "\n",
    "\n",
    "'''\n",
    "Project: Neural Network for MHC Peptide Prediction\n",
    "Class(s): BuildNetwork\n",
    "Function: Gssssnerates specified neural network architecture (data agnostic)\n",
    "\n",
    "Author: Patrick V. Holec\n",
    "Date Created: 2/2/2017\n",
    "Date Updated: 2/2/2017\n",
    "'''\n",
    "\n",
    "\n",
    "# standard libaries\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "\n",
    "# nonstandard libraries\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import load_data as loader\n",
    "\n",
    "# library modifications\n",
    "random.seed(42)\n",
    "tf.set_random_seed(42)\n",
    "\n",
    "def main():\n",
    "    data_settings = {\n",
    "                     'num_epochs':100,'learning_rate':0.001,\n",
    "                     'data_augment':False,'data_normalization':True,\n",
    "                     'data_silent':True\n",
    "                    }\n",
    "    \n",
    "\n",
    "    model = BuildModel('A12',data_settings)\n",
    "    #model.data_format()\n",
    "    #model.network_initialization()\n",
    "    #model.train()\n",
    "    #guesses = model.predict()\n",
    "    #visualize(guesses,model.test_labels)\n",
    "    \n",
    "'''\n",
    "Factory Methods\n",
    "'''\n",
    "\n",
    "# create a weight variable\n",
    "def weight_variable(shape):\n",
    "    \"\"\"Create a weight variable with appropriate initialization.\"\"\"\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "# create a bias variable\n",
    "def bias_variable(shape):\n",
    "    \"\"\"Create a bias variable with appropriate initialization.\"\"\"\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "# convolution filter function \n",
    "def conv2d(x, W, stride=1):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, stride, stride, 1], padding='SAME')\n",
    "\n",
    "# max pool function (check if lists are passed in for movement)\n",
    "def max_pool(x, stride=2, filter_size=2, padding='SAME'):\n",
    "    if not(type(stride) == list and type(filter_size) == list):\n",
    "        filter_size,stride = [1, filter_size, filter_size, 1],[1, stride, stride, 1]\n",
    "    return tf.nn.max_pool(x, ksize=filter_size,strides=stride, padding=padding)\n",
    "\n",
    "# TODO: probably not used?\n",
    "def cross_entropy(y, y_real, W1=None,W2=None,W1fc=None,W2fc=None,modifications=['NONE'],loss_type='NONE',loss_coeff=0):\n",
    "    return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y, y_real))\n",
    "\n",
    "# activation_fn\n",
    "def activation_fn(inp,fn=None,dropout=1.0):\n",
    "    if fn == 'relu': return tf.nn.dropout(tf.nn.relu(inp), dropout)\n",
    "    elif fn == 'relu6': return tf.nn.dropout(tf.nn.relu6(inp), dropout)\n",
    "    elif fn == 'sigmoid': return tf.nn.dropout(tf.nn.sigmoid(inp), dropout)\n",
    "    elif fn == 'tanh': return tf.nn.dropout(tf.nn.tanh(inp), dropout)\n",
    "    elif fn == 'softplus': return tf.nn.dropout(tf.nn.softplus(inp), dropout)\n",
    "    elif type(fn) == str: print '{} function not recognized, using default (NONE)...'.format(fn)\n",
    "    return tf.nn.dropout(inp, dropout)\n",
    "\n",
    "def network_loss(y,y_real,W,params):\n",
    "    # base loss\n",
    "    if params['loss_type'] == 'l1': loss = params['loss_magnitude']*tf.reduce_sum(tf.abs(tf.subtract(y,y_real)))\n",
    "    if params['loss_type'] == 'l2': loss = params['loss_magnitude']*tf.nn.l2_loss(tf.subtract(y,y_real))\n",
    "    else: loss = 0\n",
    "    # regularization loss\n",
    "    if params['reg_type'] == 'l1': loss += params['reg_magnitude']*tf.reduce_sum([tf.reduce_sum(tf.abs(w)) for w in W])\n",
    "    if params['reg_type'] == 'l2': loss += params['reg_magnitude']*tf.reduce_sum([tf.nn.l2_loss(w) for w in W])\n",
    "    return loss\n",
    "\n",
    "\n",
    "'''\n",
    "Main Class Method\n",
    "'''\n",
    "\n",
    "class BuildModel:\n",
    "    \n",
    "    def default_model(self):\n",
    "        # basically every parameter defined in one dictionary\n",
    "        default_params = {#'data_augment':False,\n",
    "                         'learning_rate':0.01,\n",
    "                         'data_normalization': False,\n",
    "                         'silent': False,\n",
    "                         'test_fraction': 0.1,\n",
    "                         'batch_size':100,\n",
    "                         'num_epochs':50,\n",
    "                         'loss_coeff':0.01,\n",
    "                         'learning_rate':0.01,\n",
    "                         # overall network parameters\n",
    "                         'fc_layers':2,\n",
    "                         # sitewise/pairwise parameters\n",
    "                         'pw_depth':1,\n",
    "                         'sw_depth':1,\n",
    "                         # fully connected parameters\n",
    "                         'fc_depth':(16,1),\n",
    "                         'fc_fn':('sigmoid ','sigmoid','linear'),\n",
    "                         'fc_dropout':(1.0,1.0),\n",
    "                         # loss parameters\n",
    "                         'loss_type':'l2',\n",
    "                         'loss_magnitude':1.0,\n",
    "                         # regularization parameters\n",
    "                         'reg_type':'l2',\n",
    "                         'reg_magnitude':0.01,\n",
    "                         }\n",
    "        \n",
    "        # apply all changes\n",
    "        self.update_model(default_params)\n",
    "\n",
    "    # use a dictionary to update class attributes\n",
    "    def update_model(self,params={}):\n",
    "        # makes params dictionary onto class attributes\n",
    "        for key, value in params.items():\n",
    "            setattr(self, key, value)\n",
    "        \n",
    "        # checks for adequete coverage\n",
    "        assert len(self.fc_depth) >= self.fc_layers,'Entries in depth less than number of fc layers.'\n",
    "        \n",
    "    # model initialization\n",
    "    def __init__(self,label,params = {}):\n",
    "        \n",
    "        # set all default parameters\n",
    "        self.default_model()\n",
    "        \n",
    "        # check to see if there is an update\n",
    "        if params:\n",
    "            print 'Updating model with new parameters:'\n",
    "            for key,value in params.iteritems(): print '  - {}: {}'.format(key,value)\n",
    "            self.update_model(params)\n",
    "        \n",
    "        print 'Initializing neural network data acquisition...'        \n",
    "        \n",
    "        # load data parameters\n",
    "        data = loader.LoadData(label)\n",
    "        self.__dict__.update(data.params)\n",
    "        self.all_data_sw = data.data_array_sw # load all variables\n",
    "        self.all_labels = data.label_array\n",
    "        \n",
    "        self.sw_dim,self.pw_dim = self.all_data_sw.shape,self.all_data_pw.shape # save dimensions of original data\n",
    "        self.full_size = (self.all_data_sw[0].size,self.all_data_pw[0].size) # number of entries in each input data\n",
    "        self.pairs = (self.length*(self.length-1))/2        \n",
    "        \n",
    "        # verified reduction of dimension and merging\n",
    "        self.flatten_data_sw = np.reshape(self.all_data_sw,\n",
    "                                          (self.pw_dim[0],self.sw_dim[1]*self.sw_dim[2]))\n",
    "        self.flatten_data_pw = np.reshape(self.all_data_pw,\n",
    "                                          (self.pw_dim[0],self.pw_dim[1]*self.pw_dim[2]*self.pw_dim[3]))\n",
    "        self.all_data = np.concatenate((self.flatten_data_sw,self.flatten_data_pw),axis=1)\n",
    "        \n",
    "         # update on model parameters\n",
    "        if not self.silent:\n",
    "            print '*** System Parameters ***'\n",
    "            print '  - Sequence length:',self.length\n",
    "            print '  - AA count:',self.aa_count       "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
